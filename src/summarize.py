import logging
import os
import time

from dotenv import load_dotenv
from google import genai
import textwrap

load_dotenv()

# load Gemini API Key
api_key = os.getenv("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)


class ContentSummary:
    def __init__(self, client, data):
        self.client = client
        self.data = data

    def build_prompt(self):
        """
        Build a natural language prompt for the LLM to summarize and compare content.
        """
        cleaned_text = self.data.get("cleaned_text", "")
        summary_bart = self.data.get("summary_bart", "")
        summary_t5 = self.data.get("summary_t5", "")

        prompt = textwrap.dedent(
            f"""
            You are a helpful and knowledgeable AI assistant specializing in model summarization.

            Below is a Reddit post and its summaries generated by two different transformer models
                                 (BART and T5-Large):

            Original Post:
            {cleaned_text}

            BART Summary:
            {summary_bart}

            T5-Large Summary:
            {summary_t5}

            Please:
            - Write a concise, friendly summary (2â€“3 sentences) that captures the sentiment and
            content of the post.
            - Compare the BART and T5-Large summaries with the original post.
            - Highlight any differences in content, sentiment, or style between the two summaries.
            - Do not include any personal opinions or assumptions.
            - Do not include any additional information besides the requested summary and
            model comparisons.

        """
        ).strip()

        return prompt

    def get_answer(
        self, model="gemini-2.0-flash", fallback_models=None, retries=3, backoff=2
    ):
        prompt = self.build_prompt()
        models_to_try = [model] + list(fallback_models or ())
        for m in models_to_try:
            for i in range(retries):
                try:
                    response = self.client.models.generate_content(
                        model=m, contents=prompt
                    )
                    return response.text
                except genai.errors.ServerError as e:
                    if "model is overloaded" in str(e) and i < retries - 1:
                        wait = backoff**i
                        logging.warning(
                            f"[Gemini] {m} overloaded. "
                            f"Retrying in {wait} seconds..."
                        )
                        time.sleep(wait)
                    else:
                        logging.error(f"[Gemini] Failed with model {m} with Error {e}")
                        break  # try next model
                except Exception as e:
                    logging.error(f"[Gemini] Unexpected error with model {m}: {e}")
                    break
        raise RuntimeError("All model attempts failed to generate a response.")

    @staticmethod
    def summarize_rows(client, rows, **kwargs):
        """
        Summarize and compare a list of rows (each a dict with keys:
            cleaned_text, summary_bart, summary_t5).

        Returns a list of summaries.
        """
        results = []
        for row in rows:
            cs = ContentSummary(client, row)
            try:
                summary = cs.get_answer(**kwargs)
            except Exception as e:
                summary = f"Error: {e}"
            results.append(summary)
        return results
